\begin{markdown}

\chapter{Background}
  
Programming a modern \gls{GPU} is a hard problem. A lot of
architecture specific aspects has to be taken into
consideration. Traditionaly these devices can only be programmed in
low level languages such as C/C++. Software libraries and language
bindings does exist for higher level languages. Enabling a user of
such a language make use of the device, but the kernel\footnote{A
  kernel is the piece of code that is executed in parallel, this is
  usually implemented as a function.} implementation is still in a low
level language.

The Julia programming language aims to unify the two worlds of high
productivity, given by abstractions, and high performance, given by a
staticaly compiled language. This will enable users of this seemingly
high level language to dive into optimizing their applications without
having to resort to a lower level language for the
implementation.

The PTX.jl library presented in this report tries to extend the effort
made by the Julia language to the realm of \gls{GPU} programming. This
is possible due to the fact that Julia is implemented on top of
\gls{LLVM} \cite{llvm} which also is the basis for both NVVM
\cite{nvvm} and SPIR \cite{spir}, the intermediate representations of
both the NVIDIA and the \gls{OpenCL} compiler.

# History and State of the art #

Programming a \gls{GPU} started out with a fixed pipeline with
programmable steps used for rendering 3D graphics. In 2006 with the
introduction of __\gls{CUDA}__ \cite{cuda} by NVIDIA and __\gls{APP}__
\cite{app} by AMD in 2007, \gls{GPGPU} was introduced. \gls{GPGPU}
entailes that the pipeline is fully programmable. This enables the
\gls{GPU} to be used for non-graphic related applicaion.  Microsoft
released __DirectCompute__ \cite{directcompute} along with DirectX11
in 2008 that exposes a \gls{API} for the \gls{GPU} to the programmer
in C++. In 2009 the Khronos Group released the first specification of
__\gls{OpenCL}__ \cite{opencl} which tries to provide a platform
independent language for programming heterogeneous systems. The
__OpenACC__ \cite{openac} standard was released in 2011 enabling
programmers to execute code on \gls{GPU}s with directives as OpenMP
\cite{openmp} enables multiple threads. In 2012 Microsoft released
__C++ \gls{AMP}__ \cite{c++amp}, a C++ language- and programming model
extension. Microsoft released a compiler for their own DirectCompute
framework, and Intel and AMD has later released compilers based on the
specification \cite{shelvin park} \cite{amd-amp}. __Numba__
\cite{numba} introduced by Continuum Analytics and is still in
development (2014), it enables executing Python code on a NVIDIA GPU
and is built on \gls{LLVM}.

Out of these \gls{CUDA} and \gls{OpenCL} are most suited for
considerations in this projects as they can be used as compiler
targets. The others mentioned are software libraries that build on
these underlying standards. The Numba compiler comes closest in
functionalty of the approach taken by PTX.jl.

# The SIMT model #

This section will provide a brief introduction to the programming
model exposed to the programmer for both OpenCL \footnote{In general,
  OpenCL exposes a model called \gls{SPMD} \cite{opencl}, but on a GPU
  with a underlying vector architecture this can be considered as
  SIMT.}  and CUDA. \gls{SIMT} is an abstraction over the hardware
technique of \gls{SIMD}. The model provides the programmer with the
abstraction that a kernel is executed in an independent thread. The
kernel usualy takes large one-dimentional arrays as input and output
parameter. The environment defines index functions that the programmer
can use to access elements of the data structures. An example kernel
for vector addition is given in Figure \ref{smit:add}.

\begin{figure}[H]
  \begin{minted}[linenos,numbersep=5pt,gobble=2,frame=lines,framesep=2mm]{c}
  __kernel void add(long *a, long *b, long *c) {
    int i = get_global_id(0);
    c[i] = a[i] + b[i];
  }
  \end{minted}
  \caption{OpenCL Vector addition kernel}
  \label{smit:add}
\end{figure}

On line 2 the kernel finds its index in the first dimension and on
line 3 a single element addition is performed. The _Host_ (\gls{CPU})
schedules the kernel to be executed on the same number of threads as
the length of the array. The hardware on the _Device_ (\gls{GPU})
ensures that the index function returns the correct index for each
thread.

# Intermediate Representations #

In the last few years both \gls{OpenCL} and \gls{CUDA} has started
targeting the \gls{LLVM} compiler infrastructure. NVIDIA released part
of their compiler in 2009 as open source. This included the \gls{PTX}
backend \cite{nvptx} and the libNVVM \cite{libnvvm} optimizing
compiler, both based on \gls{LLVM}. They also introduced a new
intermediate representation called \gls{NVVM} which is largely based
on \gls{LLVM} \gls{IR}. In 2012 the Khronos Group introduced
\gls{OpenCL} \gls{SPIR}, an intermediate representation also based on
\gls{LLVM} \gls{IR}. As both these \glspl{IR} was considered as
targets for the PTX.jl library, a short introduction is given in the
following subsections.

## LLVM IR ##

This section will briefly introduce the \gls{LLVM} intermediate
representation. The \gls{SPIR} and \gls{NVVM} representations
described in the next two subsections are extensions to this
representation.

\gls{LLVM} \gls{IR} is a compiler internal intermediate
representation. It is intended as a good representation for compiler
optimization and analysis. The representation has three forms, a human
readable format (.ll), a bitcode format (.bc) and an in-memory
format. These three forms are all equivalent, therefore the human
readable format is presented here.

The format is on \gls{SSA} form, which makes reasoning about the code
easy. The language consists of modules that contains a _target_,
_function declarations_ and _implementations_, _type specifications_,
_attributes_ and _metadata nodes_.

Figure \ref{fig:llvm} lists a typical \gls{LLVM} module.

\begin{figure}[H]
  \begin{minted}[linenos,numbersep=5pt,gobble=2,frame=lines,framesep=2mm]{llvm}
  ; ModuleID = 'add.bc'
  target datalayout = "e-m:o-i64:64-f80:128-n8:16:32:64-S128"
  target triple = "x86_64-apple-macosx10.9.0"
  
  ; Function Attrs: nounwind ssp uwtable
  define void @add(i64* %a, i64* %b, i64* %c) #0 {
    ; code 
    ret void
  }
  
  declare i64 @get_global_id(i32)
  
  attributes #0 = { nounwind ssp uwtable  }
  
  !llvm.ident = !{!0}
  
  !0 = metadata !{metadata !"Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn)"}
  \end{minted}
  \caption{A module with a function in LLVM IR}
  \label{fig:llvm}
\end{figure}


Lines 1 and 2 in Figure \ref{fig:llvm} specifies the underlying target
architecture with the _datalayout_ and the _target triple_. Next on
line 6 is a _function implementation_ of the _add_ function, the #0 on
the end of line binds the function to the attributes _nounwind ssp
uwtable_ specified on line 13. Above the attribute on line 11 is a _
function declaration_ of _get_global_id_. The last two lines 15 and 17
specifies metadata. LLVM contains both named metadata and unnamed. The
unnamed section gets assigned numbers !0 in this case on line 17.  The
_!llvm.ident_ on line 15 referes to this unnamed node. 

## SPIR ##

\gls{SPIR} is \glspl{OpenCL} format for representing kernels for
execution on heterogeneous systems. The specification is a mapping
from OpenCL C into LLVM IR. SPIR does not add a lot of extra to the
LLVM IR. The notable differences are new _target datalayout_ and
_triple_, a _named metadata node_ for specifying kernels,
specification of _address space qualifiers_ and _name mangling
rules_. In addition the kernel arguments must be included in the
metadata section. Figure \ref{fig:spir} lists a \gls{SPIR} module
containing the kernel definition for the _add_ function.


\begin{figure}[H]
  \begin{minted}[linenos,numbersep=5pt,gobble=2,frame=lines,framesep=2mm]{llvm}
  ; ModuleID = 'OpenCL Module'
  target datalayout = "e-p:64:64:64- ... -v1024:1024:1024" ; Not complete
  target triple = "spir64-unknown-unknown"
  
  declare i64 @_Z13get_global_idj(i32)
  
  define void @add(i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*) {
    ; code
    ret void
  }
  
  !llvm.ident = !{!0}
  !opencl.kernels = !{!1}
  
  !0 = metadata !{metadata !"Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5  svn)"}
  !1 = metadata !{void (i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*)* @add}
  \end{minted}
  \caption{A module with a kernel definition in SPIR}
  \label{fig:spir}
\end{figure}

### The kernels metadata node ###

All SPIR modules must contain the named metadata node
__opencl.kernels__. The value of this node is a list of the kernel
functions in the module. For each kernel the signature must be
included in the metadata section. As seen in Figure \ref{fig:spir} the
opencl.kernels node points to the method signature in _!1_ and this
node matches the signature of the function.

### Address Space Qualifiers ###

The qualifier used in the parameter list of the add functions shows
how address space qualifiers are utilized in SPIR. This qualifiers
denotes what type of memory is used on the device.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    Qualifier & Memory   & Comment \\
    \hline
    \hline
    0         & private  & Private to each work item \\
    \hline
    1         & global   & Global for the entire device \\
    \hline
    2         & constant & Global for the entire device \\
    \hline
    3         & local    & Private to each work group \\
    \hline
    4         & generic  & TODO: I dunno \\
    \hline
  \end{tabular}
  \caption{Address Space Qualifiers for SPIR}
  \label{tab:spir:addr}
\end{table}
  
The example in Figure \ref{fig:spir} uses only the global memory
space.

### Name Mangling ###

OpenCL contains many builtin overloaded functions like the math
function __sin__. Name mangling is used to distinguish between the
different implementations based on their argument types. In Figure
\ref{fig:spir} on line 5 the _get_global_id_ is mangled into
__Z13get_global_idj_, the mangling rules are based on the rules for
the Intel Itanium.

## NVVM ##

\gls{NVVM} is NVIDIAs intermediate representation, used in their
\gls{LLVM} based compiler for \gls{CUDA}. It does as \gls{SPIR},
represent device kernels that are intended to be executed on a device,
but solely \glspl{GPU}. \gls{NVVM} adds to \gls{LLVM} \gls{IR} a set of
intrinsic functions for controlling the \gls{GPU}. Among these are
barriers, address space conversions, special register accessors. The
\gls{NVVM} Cuda Compiler compiles \gls{NVVM} \gls{IR} into \gls{PTX}
which is loaded into a the devices with either the \gls{CUDA} or
\gls{OpenCL} driver provided by NVIDIA. Like \gls{SPIR} it defines
targets, address space qualifiers and a metadata node to declare a
function as a kernel.

\begin{figure}[H]
  \begin{minted}[linenos,numbersep=5pt,gobble=2,frame=lines,framesep=2mm]{llvm}
  ; ModuleID = 'OpenCL Module'
  target datalayout = "e-p:32:32:32- ... -v1024:1024:1024" ; Not complete
  target triple = "nvptx64-nvidia-cuda"
  
  declare i32 @llvm.nvvm.read.ptx.sreg.tid.x()
  
  define void @add(i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*) {
    ; code
    ret void
  }
  
  !llvm.ident = !{!0}
  !nvvm.annotations = !{!1}
  
  !0 = metadata !{metadata !"Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn)"}
  !1 = metadata !{void (i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*)* @add,
  metadata !"kernel", i32 1}
  \end{minted}
  \caption{A module with a kernel definition in NVVM}
  \label{fig:nvvm}
\end{figure}

### NVVM Annotations ###

The metadata node __nvvm.annotations__ explicitly marks the node as a
kernel node on line 17 in figure \ref{fig:nvvm}. This is done as the
annontations is also used for global variables etc. These are not used
in the PTX.jl library.

### Builtin Intrinsics ###

\gls{NVVM} comes with a set of built in intrisics functions. The
definition of the _threadIdx.x_ variable, accessible inside \gls{CUDA}
kernels, is implemented as such and intrinsic. Its definition is given
at line 5 of Figure \ref{fig:nvvm}.

### Address Space Qualifiers ###

Like \gls{SPIR}, \gls{NVVM} specifies address space qualifiers. They
differ only slightly from the OpenCL definition and is more tailored
to the NVIDIA GPU architecture. Table \ref{tab:nvvm:addr} tabulates
the different qualifiers and their usage.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|l|}
    \hline
    Qualifier & Memory   & Comment                      & OpenCL equivalent \\
    \hline
0         & generic  &                   & generic  \\
1         & global   & Global for the entire device & global \\
3         & shared   & Private to each work group   & local \\
4         & constant & Global for the entire device & constant \\
5         & local    & Private to each work item    & private \\
    \hline
  \end{tabular}
  \caption{Address Space Qualifiers for NVVM}
  \label{tab:nvvm:addr}
\end{table}


## Summary ## 

As the above discussion shows, SPIR and NVVM are in the gist very
similar. They both define multiple address spaces, builtin
functionality and a way to distingush kernel functions. The lagest
incompability is the definition of the address qualifiers. 

## Support ##

Both these intermediate representations are inteded as compiler
targets for new programming languages. The NVVM is only supported by
NVIDIAs drivers and can only be executed on NVIDIA GPUs. SPIR on the
other hand can potentialy execute on all platforms that execute
OpenCL. Unfortunalty at the time of writing a implementation on NVIDIA
is not available, but both Intel an AMD has support for SPIR in their
newest hardware.

# A governing factor in Hardware Platform #

An external governing factor for the PTX.jl implementation has been the
available hardware for testing. The workstation listed in Table
\ref{tab:hw} was provided in the NTNU
HPC-Lab\footnote{http://research.idi.ntnu.no/hpc-lab/} by Dr. Anne
C. Elster.


\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    & CPU & GPU \\
    \hline
    \hline
    Name & Intel i7 & NVIDIA GTX 480 \\
    \hline
    \# of Cores & 4 & 448 \\
    \hline
    Memory & 4GB & 1536MB \\
    \hline
  \end{tabular}
  \caption{Hardware platform}
  \label{tab:hw}
\end{table}

As this platform consists of a NVIDIA GPU the IR chosen was NVVM. As
the outlines in this section implies, changing the library to target
SPIR instead can be easily done.

\end{markdown}

\begin{markdown}

\chapter{Background}
  
Programming a modern GPU is a hard problem. A lot of architecture
specific aspects has to be taken into consideration. Traditionaly
these devices can only be programmed in low level languages such as
C/C++. Software libraries and language bindings does exist for higher
level languages. Enabling a user of such a language make use of the
device, but the core implementation is still in a low level language.

The Julia programming language aims to unify the two worlds of high
productivity, given by abstractions, and high performance, given by a
staticaly compiled language. This will enable users of this seemingly
high level language to dive into optimizing their applications without
having to resort to a lower level language for the
implementation.

This project tries to do the same in the realm of GPU
programming. This is possible due to the fact that Julia is
implemented on top of LLVM which also is the basis for both NVVM and
SPIR, the intermediate representations of both the NVIDIA and the
OpenCL compiler.

# State of the art #

Programming a GPU started out with a fixed pipeline with programmable
steps used for rendering 3D graphics. In 2006 with the introduction of
__CUDA__ by NVIDIA and __APP__ by AMD in 2007, __GPGPU__ was
introduced. GPGPU is short for General Purpose Graphical Processing
Unit and entailes that the pipeline is fully programmable. Microsoft
released __DirectCompute__ along with DirectX11 in 2008 that exposes a
API for the GPU to the programmer in C++. In 2009 the Khronos Group
released the first specification of __OpenCL__ which tries to provide
a platform independent language for programming heterogeneous
systems. The __OpenACC__ standard was released in 2011 enabling
programmers to execute code on GPUs with directives as OpenMP enables
multiple threads. In 2012 Microsoft released __C++ AMP__, a C++
language and programming model extension. Microsoft released a
compiler for their own __DirectCompute__ framework, and Intel and AMD
has later released compilers based on the specification. __Numba__
introduced by Continuum Analytics and is still in development (2014),
it enables executing Python code on a NVIDIA GPU.

Out of these CUDA and OpenCL are most suited for considerations in
this projects as they can be used as compiler targets. The others
mentioned are software libraries that build on these underlying
standards.

# The SIMT model #

This section will provide a brief introduction to the programming
model (SIMT) exposed to the programmer for both OpenCL \footnote{In
  general, OpenCL exposes a model called Single Program Multiple Data
  \cite{}, but on a GPU with a underlying vector architecture this can
  be considered as SIMT.} and CUDA. SIMT, Single Instruction Mutliple
Threads, is an abstraction over the hardware technique of SIMD, Single
Instruction Multiple Data. The model provides the programmer with the
abstraction that a kernel is executed in an independent thread. The
kernel usualy takes large one-dimentional arrays as input and output
parameter. The environment defines index functions that the programmer
can use to access elements of the data structures. An example kernel
for vector addition is given in Figure \ref{smit:add}


\begin{figure}[H]
  \begin{minted}{c}
__kernel void add(long *a, long *b, long *c) {
  int i = get_global_id(0);
  c[i] = a[i] + b[i];
}
  \end{minted}
  \caption{OpenCL Vector addition kernel}
  \label{smit:add}
\end{figure}


# Intermediate Representations #

In the last few years both OpenCL and CUDA has started targeting the
LLVM compiler infrastructure. NVIDIA released part of their compiler
in 2009 as open source. This included the PTX backend and the libNVVM
optimizing compiler, both based on LLVM. They also introduced a new
intermediate representation called NVVM which is largely based on LLVM
IR. In 2012 the Khronos Group introduced OpenCL SPIR, a intermediate
representation also based on LLVM IR. As both these IRs was considered
as targets for this project a short introduction is given is the
following subsections. 

## LLVM IR ##

This section will introduce the LLVM intermediate representation
briefly as the representations described in the next to subsections
are extensions to it.

LLVM IR is a compiler internal intermediate representation. It is
intended as a good representation for compiler optimization and
analysis. The representation has three forms, a human readable format
(.ll), a bitcode format (.bc) and an in-memory format. These three
forms are all equivalent, therefore the human readable format is
presented here.

The format is SSA, Static Single Assignment which makes reasoning
about the code easy. The language consists of modules that contains a
target, function declarations and implementations, type
specifications, attributes and metadata.

This is a typical LLVM program:

\begin{minted}{llvm}
; ModuleID = 'add.bc'
target datalayout = "e-m:o-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-apple-macosx10.9.0"

; Function Attrs: nounwind ssp uwtable
define void @add(i64* %a, i64* %b, i64* %c) #0 {
  ; code 
  ret void
}

declare i64 @get_global_id(i32)

attributes #0 = { nounwind ssp uwtable  }

!llvm.ident = !{!0}

!0 = metadata !{metadata !"Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn)"}

\end{minted}


The first two lines specifies the underlying target architecture with
_datalayout_ and _target triple_. Next is a function
implementation of the _add_ function, the _#0_ binds the function
to the attributes _nounwind ssp uwtable_ specified on line 13. Next a
declaration of _get_global_id_ follows. The last two lines specifies
metadata. LLVM contains both named metadata and unnamed. The unnamed
section gets assigned numbers _!0_ in this case.  The _!llvm.ident_
referes to this unnamed node. We see alot of referals in the metadata
section of LLVM modules.


## SPIR ##

SPIR is OpenCLs new format for representing kernel for execution on
heterogeneous systems. The specification is a mapping from OpenCL C
into LLVM IR. SPIR does not add a lot of extra to the LLVM IR. The
notable differences are _target datalayout_ and _triple_, a _named
metadata node_ for specifying kernels, specification of _address space
qualifiers_ and _name mangling rules_. The kernel argument information is
carried in the metadata section.

\begin{minted}{llvm}
; ModuleID = 'OpenCL Module'
target datalayout = "e-p:64:64:64- ... -v1024:1024:1024" ; Not complete
target triple = "spir64-unknown-unknown"

declare i64 @get_global_id(i32)

define void @add(i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*) {
  ; code
  ret void
}

!llvm.ident = !{!0}
!opencl.kernels = !{!1}

!0 = metadata !{metadata !"Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn)"}
!1 = metadata !{void (i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*)* @add}
\end{minted}

### The kernels metadata node ###
All SPIR modules must contain the named metadata node
__opencl.kernels__. The value of this node is a list of the kernel
functions in the module. For each kernel the signature must be
included in the metadata section. As seen above the opencl.kernels
node points to the method signature in _!1_ and this node matches the
signature of the function.

### Address Space Qualifiers ###
The qualifier used in the parameter list of the add functions shows
how address space qualifiers are utilized in SPIR. This qualifiers
denotes what type of memory is used on the device.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    Qualifier & Memory   & Comment \\
    \hline
    \hline
    0         & private  & Private to each work item \\
    \hline
    1         & global   & Global for the entire device \\
    \hline
    2         & constant & Global for the entire device \\
    \hline
    3         & local    & Private to each work group \\
    \hline
    4         & generic  & TODO: I dunno \\
    \hline
  \end{tabular}
\end{table}
  
The example above uses only the global memory space.

### Name Mangling ###

OpenCL contains many builtin overloaded functions like the math
function __sin__. Name mangling is used to distinguish between the
different implementations based on their argument types. In the
example above the _get_global_id_ is mangled into
__Z13get_global_idj_, the mangling rules are based on the rules for
the Intel Itanium.

## NVVM ##

NVVM is NVIDIAs intermediate representation used in their LLVM based
compiler for CUDA. It does as SPIR, represent device kernels that are
intended to be executed on a device, but solely GPUs. NVVM ads to LLVM
IR a set of intrinsic functions for controlling the GPU. Among these
are barriers, address space conversions, special register
accessors. The NVVM Cuda Compiler compiles NVVM IR into PTX which is
loaded into a the devices with either the CUDA or OpenCL driver
provided by NVIDIA. Like SPIR it defines targets, address space
qualifiers and a metadata node to declare a function as a kernel.

\begin{minted}{llvm}
; ModuleID = 'OpenCL Module'
target datalayout = "e-p:32:32:32- ... -v1024:1024:1024" ; Not complete
target triple = "nvptx64-nvidia-cuda"

declare i32 @llvm.nvvm.read.ptx.sreg.tid.x()

define void @add(i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*) {
  ; code
  ret void
}

!llvm.ident = !{!0}
!nvvm.annotations = !{!1}

!0 = metadata !{metadata !"Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn)"}
!1 = metadata !{void (i64 addrspace(1)*, i64 addrspace(1)*, i64 addrspace(1)*)* @add,
  metadata !"kernel", i32 1}
\end{minted}

### NVVM Annotations ###
The metadata node _nvvm.annotations_ explicitly marks the node as a
kernel node. This is done as the annontations is also used for global
variables etc. 

### Builtin Intrinsics ###
NVVM comes with a set of built in intrisics functions. The definition
of the threadIdx.x variable accessible inside CUDA kernels is
implemented as such and intrinsic. Its definition is showed in the
example above the add function. 

### Address Space Qualifiers ###
Like SPIR NVVM specifies address space qualifiers. The differ only
slightly from the OpenCL definition and is more tailored to the NVIDIA
GPU architecture.
\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|l|}
    \hline
    Qualifier & Memory   & Comment                      & OpenCL equivalent \\
    \hline
0         & generic  & TODO: dunno                  & generic?  \\
1         & global   & Global for the entire device & global \\
3         & shared   & Private to each work group   & local \\
4         & constant & Global for the entire device & constant \\
5         & local    & Private to each work item    & private \\
    \hline
  \end{tabular}
\end{table}


## Summary ## 

As the above discussion shows, SPIR and NVVM are in the gist very
similar. They both define multiple address spaces, builtin
functionality and a way to distingush kernel functions.

## Support ##

Both these intermediate representations are inteded as compiler
targets for new programming languages. The NVVM is only supported by
NVIDIAs drivers and can only be executed on NVIDIA GPUs. SPIR on the
other hand can potentialy execute on all platforms that execute
OpenCL. Unfortunalty at the time of writing a implementation on NVIDIA
is not available, but both Intel an AMD has support for SPIR in their
newest hardware.

# A governing factor in Hardware Platform #

A external governing factor for the PTX.jl implementation has been the
available hardware for testing. The workstation listed in Table
\ref{tab:hw} was provided in the NTNU
HPC-Lab\footnote{http://research.idi.ntnu.no/hpc-lab/} by Dr. Anne
C. Elster.


\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    & CPU & GPU \\
    \hline
    \hline
    Name & Intel i7 & NVIDIA GTX 480 \\
    \hline
    \# of Cores & 4 & 448 \\
    \hline
    Memory &  &  \\
    \hline
  \end{tabular}
  \caption{}
  \label{tab:hw}
\end{table}

As this platform consists of a NVIDIA GPU the IR chosen were NVVM. As
the outlines in this section implies, changing the library to target
SPIR instead can be easily done.

\end{markdown}

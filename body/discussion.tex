\chapter{Discussion}
\begin{markdown}

The contents of this chapter discusses the design and implementation
of the PTX.jl described in Chapters \ref{chap:impl} and
\ref{chap:meth}, and the results presented in Chapter \ref{chap:res}.

# Design #

## PTX.jl as a library ##

As mentioned in Section \ref{sec:meth:lib-b-comp} the decision to
implement PTX.jl as a library was not the most obvious one. This was
not a clear decision taken up front, but more a alternative that
materialized when the changes to compiler was removed due to library
level alternatives. The most improtant parts was the facts that both
inlining could be disabled and implementing multiple datatypes
primitive datatypes was made possible. The library can serve as a
basis to future investigate new patterns for GPU programming in Julia
without having modify the compiler.

## Unboxing of arrays ##

The approach mentioned in Section \ref{sec:meth:arrays:low-level}, the
unboxing of arrays was considered an easy first iteration step in
order to establish the whole prototype system. This prototype can now
be extended with more sophisticated translation from Julia to PTX
code. The performance results depicted in Section \ref{sec:res} can
serve as a benchmark for future higher level implementations. 

# Implementation #

## Fragile kernel compiler ##
\label{sec:disc:comp}
The kernel compiler in PTX.jl is rather fragile. Introducing to much
complexity in the julia kernel function will cause either the _module
generator_ or the _lowering compiler_ to fail in a non-user-friendly
fashion.

# Result #

## How is PTX.jl holding up ##

We see from both benchmarks presented in Chapter \ref{chap:res} that
the performance of the kernel produced by PTX.jl is comparable to both
OpenCL C and CUDA C. This indicates that the method of lowering
incures no overhead over the kernels developed in a lower level
language. 

## Offloading work to the GPU ##

The micro benchmark presented in the Section \ref{sec:res:mm}
indicates that offloading even relatively small matrix computations to
the GPU will provide a speedup. This even when the implementations
presented in this reported are unoptimized for perforamance. Usually
when looking to exploit the parallelizm of a GPU, the entire
application is implemented on the GPU. Given the results there is
reason to explore a more fine-grained parallelization of a program. A
proposition for a framework for this is presented in the next
Subsection.

### Framework for fine-grained offloading ###

Building on the _GPUArray_ type presented in Section
\ref{sec:meth:ptx} a framework for fine-grained offloading can be
built. Extending the type with operators and methods for Matrix and or
Vector operations. The operations can also have built in autotuning to
decide when to offload to the GPU and when to keep the computation
on the CPU.

## Integer vs. Floating point performance in Matrix Multiplication ##

Summarizing the results in Section \ref{sec:res:mm} we see that the
floating point perforamance of the CPU is far better than the
interger. The degration of integer mutliply performance on the CPU
enables a speedup with a naive GPU implementation already at $38*38$
matrices for 64-bit integers values. This difference seems to only
come from the fact that matrix multiplication scales badly on the CPU,
the GPU implementation scales equaly well on 

TODO
- Why does OpenCL have lower performance for Blur


## Notes on bias when timing a dynamic programming language ##

When timing benchmarks a lot of parameters might effect the
results. Special for the timings of the results presented here are
mainly the three factors \gls{GC}, \gls{JIT} and Asynchronous
Dispatch.

### Garbage Collection ###

In a garbage collected language this mechanism must be assumed to
possibly trigger at any time. There exists different types of
collection algorithms and they will affect the results in different
ways. In the Julia language we have two mechanism to remove this
biased from our results. Firsly the timing macro _@time_ reports the
prosentage of time spend in the \gls{GC}. By ensuring that this number
is zero we elimintate the overhead. This can be ensured by the other
mechanism, namely the _gc()_ function. This function will run a
blocking garbage collection iteration. By inserting this before the
timed function call, we can prevent the \gls{GC} to be triggered while
timing, unless the function call it self allocates and deallocates
enough memory to be subject to \gls{GC}.


\begin{figure}
  \begin{minted}{julia}
gc()
@time foo()    
  \end{minted}
  \caption{Running Garbage Collection proir to timing}
\end{figure}

### Just-In-Time compiling ###

The Julia programming language is implemented with a \gls{JIT}. This
means that the functions are compiled when it is invoked the first
time. As we do not want to time the Julia \gls{JIT} compilation we
need to force the system to \gls{JIT} compile the function before we
time the function. In Julia this is done by simply calling the
function once. This is done prior to timing.


\begin{figure}
  \begin{minted}{julia}
foo()
# ...
@time foo()
  \end{minted}
  \caption{Triggering JIT before timing}
\end{figure}

### Timing Asynchronous Dispatch ###

Both the \gls{CUDA} and \gls{OpenCL} runtime libraries provides
functions for moving data to and from the GPU and executing the
kernels on the \gls{GPU}. These functions, except copy from GPU to CPU
are implemented as asynchronous calls, so that the GPU and CPU can
work concurrently. This means that timing a execute kernel call alone
will not actually time the execution of the kernel. To ensure that we
get the execution time we insert a synchronization point after the
function and time these two together. See Figure \ref{fig:disc:async-call}


\begin{figure}[H]
  \begin{subfigure}{.49\textwidth}
    \begin{minted}{julia}
@time Driver.execute(kernel)
    \end{minted}
    \caption{Timing Schedule Kernel}
    \label{fig:disc:async-call:sched}
  \end{subfigure}

  \begin{subfigure}{.49\textwidth}
    \begin{minted}{julia}
@time begin
  Driver.execute(kernel)
  Driver.synchronize()
end
    \end{minted}
    \caption{Timing Execution Time}
    \label{fig:disc:async-call:exec}
  \end{subfigure}

  \caption{Timing Asynchronous call}
  \label{fig:disc:async-call}
  
\end{figure}

\end{markdown}
